{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2f436c2-e026-4227-8a5c-70805808efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import lightning as L\n",
    "\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.readers import InputExample\n",
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances, paired_euclidean_distances, paired_manhattan_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b2de9bf-8747-4a57-8885-68dc11b4ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7fd5c4c-cfe6-47ca-b6ef-978be0956c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import BinaryClassificationEvaluator\n",
    "import csv\n",
    "class CustomEvaluator(BinaryClassificationEvaluator):\n",
    "    def __call__(self, model, output_path: str = None, epoch: int = -1, steps: int = -1) -> float:\n",
    "        scores = self.compute_metrices(model)\n",
    "        main_score = max(scores[short_name][\"accuracy\"] for short_name in scores)\n",
    "        file_output_data = [epoch, steps]\n",
    "        for header_name in self.csv_headers:\n",
    "            if \"_\" in header_name:\n",
    "                sim_fct, metric = header_name.split(\"_\", maxsplit=1)\n",
    "                file_output_data.append(scores[sim_fct][metric])\n",
    "        if output_path is not None and self.write_csv:\n",
    "            csv_path = os.path.join(output_path, self.csv_file)\n",
    "            if not os.path.isfile(csv_path):\n",
    "                with open(csv_path, newline=\"\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(self.csv_headers)\n",
    "                    writer.writerow(file_output_data)\n",
    "            else:\n",
    "                with open(csv_path, newline=\"\", mode=\"a\", encoding=\"utf-8\") as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(file_output_data)\n",
    "        return main_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71b89a54-b0ef-4604-b7c3-b0e0270d374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "N_SPLIT = 5\n",
    "BATCH_SIZE = 48\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cfb63d0-4a6b-451c-aebc-84f61365a52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eec78e37-91e3-48d7-b2a0-f3fdba69e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('sample_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a689313d-5fbc-4cb3-bc00-434069589f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['code1_problem'] = train_df['code1_path'].apply(lambda x: int(os.path.basename(x).split('_')[0].split('problem')[1]))\n",
    "train_df['code2_problem'] = train_df['code2_path'].apply(lambda x: int(os.path.basename(x).split('_')[0].split('problem')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31156e1b-8aa4-463d-b53e-aa5d2497645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_texts = defaultdict(list)\n",
    "# code_paths = glob('train_code/*/*.cpp')\n",
    "# for code_path in tqdm(code_paths):\n",
    "#     code_basename = os.path.basename(code_path)\n",
    "#     label = int(code_basename.split('_')[0].split('problem')[1])\n",
    "#     with open(code_path, 'r', encoding='utf-8') as f:\n",
    "#         code_text = f.read()\n",
    "#     label_texts[label].append(code_text)\n",
    "# with open('./preproc/label_texts.pkl', 'wb') as f:\n",
    "#     pickle.dump(label_texts, f)\n",
    "    \n",
    "with open('./preproc/label_texts.pkl', 'rb') as f:\n",
    "    label_texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c18bfc84-6518-40ee-b7ed-873f751de1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(list(label_texts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0df845ed-d36b-4608-8ff1-ba97e1d859c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name microsoft/codereviewer. Creating a new one with MEAN pooling.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e313487f888d458c9f76d12e7a2cd673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c526635a074270a5f6079aa4d92853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/4167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b047f74c9d114c3280f645d6e8512f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14fbf94fd2f6408494260c67ee1c295c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/4167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb86394ab26413e840be022edbf2f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6cf166645b44dfa830a1a8383cb8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/4167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=N_SPLIT,shuffle=True, random_state=SEED)\n",
    "for fold_idx, (train_index, val_index) in enumerate(kf.split(labels)):\n",
    "    # fold마다 oom이 발생해서 커널을 다시 시작해야합니다.\n",
    "    # if fold!=0: continue\n",
    "    labels_train_fold = labels[train_index]\n",
    "    labels_val_fold = labels[val_index]\n",
    "    val_df = train_df[train_df['code1_problem'].isin(labels_val_fold) & train_df['code2_problem'].isin(labels_val_fold)]\n",
    "    label_min = min((val_df['similar'] == 0).sum(),(val_df['similar'] == 1).sum())\n",
    "    val_df = pd.concat([val_df[val_df['similar']==0].sample(label_min),val_df[val_df['similar']==1].sample(label_min)],axis=0)\n",
    "\n",
    "    train_examples = []\n",
    "    for label_train in labels_train_fold:\n",
    "        for code_text in label_texts[label_train]:\n",
    "            train_examples.append(InputExample(texts=[code_text], label=label_train))\n",
    "    \n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    val_evaluator = CustomEvaluator(\n",
    "        sentences1=val_df['code1'].values.tolist(), \n",
    "        sentences2=val_df['code2'].values.tolist(), \n",
    "        labels=val_df['similar'].values.tolist(),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        show_progress_bar=True,\n",
    "        write_csv=True,\n",
    "    )\n",
    "\n",
    "    model = SentenceTransformer('microsoft/codereviewer')\n",
    "    model.forward = torch.compile(model.forward, mode=\"reduce-overhead\")\n",
    "    train_loss = losses.BatchHardSoftMarginTripletLoss(model=model)\n",
    "    model.fit(\n",
    "        use_amp=True,\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=EPOCHS,\n",
    "        warmup_steps=len(train_examples)//BATCH_SIZE,\n",
    "        save_best_model=True,\n",
    "        evaluator=val_evaluator,\n",
    "        output_path=f'./checkpoints/codereviewer-{fold_idx=}',\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77bf173f-c033-410f-a774-0d965ad08e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d10b42f9f54bfabf17798a5b050cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4758 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1125b15e4da84213a8cf287bc5e9175d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4758 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5cd80fc655d4dad81f12b390fc7c208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4758 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78ba55c5c2b461b8496c5422f4d9a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4758 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf3ae8fbd024474a82677e3479ac640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4758 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "sentences1, sentences2 = test_df['code1'].values.tolist(), test_df['code2'].values.tolist()\n",
    "sentences = list(set(sentences1 + sentences2))\n",
    "\n",
    "preds = []\n",
    "for fold_idx in range(5):\n",
    "    model = SentenceTransformer(f'./checkpoints/codereviewer-{fold_idx=}')\n",
    "    embeddings = model.encode(\n",
    "        sentences, batch_size=BATCH_SIZE, show_progress_bar=True, convert_to_numpy=True\n",
    "    )\n",
    "    emb_dict = {sent: emb for sent, emb in zip(sentences, embeddings)}\n",
    "    embeddings1 = [emb_dict[sent] for sent in sentences1]\n",
    "    embeddings2 = [emb_dict[sent] for sent in sentences2]\n",
    "    \n",
    "    score_names = ['cossim_accuracy','manhattan_accuracy','euclidean_accuracy','dot_accuracy']\n",
    "    eval = pd.read_csv(f'./checkpoints/codereviewer-{fold_idx=}/eval/binary_classification_evaluation_results.csv')\n",
    "    max_score_name = score_names[eval[score_names].max().argmax()]\n",
    "    max_score_threshold = eval.iloc[eval[score_names].max(1).values.argmax()][f\"{max_score_name}_threshold\"]\n",
    "    \n",
    "    if max_score_name == 'cossim_accuracy':\n",
    "        cosine_scores = 1 - paired_cosine_distances(embeddings1, embeddings2)\n",
    "        pred = (cosine_scores>max_score_threshold) * 1\n",
    "    elif max_score_name == 'manhattan_accuracy':\n",
    "        manhattan_distances = paired_manhattan_distances(embeddings1, embeddings2)\n",
    "        pred = (manhattan_distances<max_score_threshold) * 1\n",
    "    elif max_score_name == 'euclidean_accuracy':\n",
    "        euclidean_distances = paired_euclidean_distances(embeddings1, embeddings2)\n",
    "        pred = (euclidean_distances<max_score_threshold) * 1\n",
    "    elif max_score_name == 'dot_accuracy':\n",
    "        embeddings1_np = np.asarray(embeddings1)\n",
    "        embeddings2_np = np.asarray(embeddings2)\n",
    "        dot_scores = [np.dot(embeddings1_np[i], embeddings2_np[i]) for i in range(len(embeddings1_np))]\n",
    "        pred = (dot_scores>max_score_threshold) * 1\n",
    "    else:\n",
    "        raise ValueError\n",
    "    preds.append(pred)\n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b575d27-aa21-4bce-90f3-75c57a7d33e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68b544cc-ceed-4fb9-890c-37331aae6947",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['similar'] = (np.mean(preds,0)>0.5)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56aa81b9-714e-4f51-9933-439ae03818a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4120050420168067"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['similar'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b78d0700-45fe-4169-bb7c-5ab57615a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./test_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
