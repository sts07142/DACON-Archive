{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "199be930-7e11-43f4-b056-a52fc1a93dc8",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94dc6b5c-f92a-403d-a6f0-599f712dd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup, AutoModelForMaskedLM\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import combinations\n",
    "from rank_bm25 import BM25L\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import os, re\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bca8fd",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3efd328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 아무 내용이 없는 줄은 제거합니다. '''\n",
    "def get_rid_of_empty(c):\n",
    "    ret = []\n",
    "    splitted = c.split('\\n')\n",
    "    for s in splitted:\n",
    "        if len(s.strip()) > 0:\n",
    "            ret.append(s)\n",
    "    return '\\n'.join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9b913a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 데이터 클리닝 '''\n",
    "def clean_data(script, data_type=\"dir\"):\n",
    "    if data_type == \"dir\":\n",
    "        with open(script, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            preproc_lines = []\n",
    "            for line in lines:\n",
    "                if line.lstrip().startswith('//'):\n",
    "                    continue\n",
    "                line = line.rstrip()\n",
    "                if '//' in line:\n",
    "                    line = line[:line.index('//')]\n",
    "                line = line.replace('\\n', '')\n",
    "                line = line.replace('    ', '\\t')\n",
    "                if line == '':\n",
    "                    continue\n",
    "                preproc_lines.append(line)\n",
    "\n",
    "    elif data_type == \"file\":\n",
    "        # Split the script into lines\n",
    "        lines = script.split('\\n')\n",
    "        preproc_lines = []\n",
    "        for line in lines:\n",
    "            # Skip lines that start with '#'\n",
    "            if line.lstrip().startswith('//'):\n",
    "                continue\n",
    "            line = line.rstrip()\n",
    "            # Remove comments after '#'\n",
    "            if '//' in line:\n",
    "                line = line[:line.index('//')]\n",
    "            line = line.replace('\\n', '')\n",
    "            line = line.replace('    ', '\\t')\n",
    "            # Skip empty lines\n",
    "            if line == '':\n",
    "                continue\n",
    "            preproc_lines.append(line)\n",
    "\n",
    "    # Join the preprocessed lines into a single string\n",
    "    preprocessed_script = '\\n'.join(preproc_lines)\n",
    "    \n",
    "    # Remove /* and */ and their contents\n",
    "    preprocessed_script = re.sub(r'/\\*.*?\\*/', \"\", preprocessed_script, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove single quoted strings\n",
    "    preprocessed_script = re.sub(r'\\'\\w+', '', preprocessed_script)\n",
    "    \n",
    "    # Remove alphanumeric words\n",
    "    preprocessed_script = re.sub(r'\\w*\\d+\\w*', '', preprocessed_script)\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    preprocessed_script = re.sub(r'\\s{2,}', ' ', preprocessed_script)\n",
    "    \n",
    "    # Remove spaces around non-word characters\n",
    "    preprocessed_script = re.sub(r'\\s[^\\w\\s]\\s', '', preprocessed_script)\n",
    "\n",
    "    ''' 극소수지만 데이터 몇개는 완성되지 않은 주석들이 있었습니다 '''\n",
    "    splitted = preprocessed_script.split('\\n')\n",
    "\n",
    "    found_triple = False\n",
    "    start_idx, end_idx = -1, -1\n",
    "    for i in range(len(splitted)):\n",
    "        if found_triple == False and '/*' in splitted[i]:\n",
    "            found_triple = True\n",
    "            start_idx = i\n",
    "        elif found_triple == True and '*/' in splitted[i]:\n",
    "            end_idx = i\n",
    "    if start_idx != -1 and end_idx != -1:\n",
    "        splitted = splitted[:start_idx] + splitted[end_idx + 1:]\n",
    "    elif start_idx != -1 and end_idx == -1:\n",
    "        splitted = splitted[start_idx + 1:]\n",
    "\n",
    "    preprocessed_script = '\\n'.join(splitted)\n",
    "    preprocessed_script = get_rid_of_empty(preprocessed_script)\n",
    "        \n",
    "    return preprocessed_script\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1aba78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' positive, negative 페어 생성 함수 '''\n",
    "def get_pairs(input_df, tokenizer):\n",
    "    codes = input_df['code'].to_list()\n",
    "    problems = input_df['problem_num'].unique().tolist()\n",
    "    problems.sort()\n",
    "\n",
    "    tokenized_corpus = [tokenizer.tokenize(code) for code in codes]\n",
    "    bm25 = BM25L(tokenized_corpus)\n",
    "\n",
    "    total_positive_pairs = []\n",
    "    total_negative_pairs = []\n",
    "\n",
    "    for problem in tqdm(problems):\n",
    "        solution_codes = input_df[input_df['problem_num'] == problem]['code']\n",
    "        positive_pairs = list(combinations(solution_codes.to_list(),2))\n",
    "\n",
    "        solution_codes_indices = solution_codes.index.to_list()\n",
    "        negative_pairs = []\n",
    "\n",
    "        first_tokenized_code = tokenizer.tokenize(positive_pairs[0][0])\n",
    "        negative_code_scores = bm25.get_scores(first_tokenized_code)\n",
    "        negative_code_ranking = negative_code_scores.argsort()[::-1] # 내림차순\n",
    "        ranking_idx = 0\n",
    "\n",
    "        for solution_code in solution_codes:\n",
    "            negative_solutions = []\n",
    "            while len(negative_solutions) < len(positive_pairs) // len(solution_codes):\n",
    "                high_score_idx = negative_code_ranking[ranking_idx]\n",
    "\n",
    "                if high_score_idx not in solution_codes_indices:\n",
    "                    negative_solutions.append(input_df['code'].iloc[high_score_idx])\n",
    "                ranking_idx += 1\n",
    "\n",
    "            for negative_solution in negative_solutions:\n",
    "                negative_pairs.append((solution_code, negative_solution))\n",
    "\n",
    "        total_positive_pairs.extend(positive_pairs)\n",
    "        total_negative_pairs.extend(negative_pairs)\n",
    "\n",
    "    pos_code1 = list(map(lambda x:x[0],total_positive_pairs))\n",
    "    pos_code2 = list(map(lambda x:x[1],total_positive_pairs))\n",
    "\n",
    "    neg_code1 = list(map(lambda x:x[0],total_negative_pairs))\n",
    "    neg_code2 = list(map(lambda x:x[1],total_negative_pairs))\n",
    "\n",
    "    pos_label = [1]*len(pos_code1)\n",
    "    neg_label = [0]*len(neg_code1)\n",
    "\n",
    "    pos_code1.extend(neg_code1)\n",
    "    total_code1 = pos_code1\n",
    "    pos_code2.extend(neg_code2)\n",
    "    total_code2 = pos_code2\n",
    "    pos_label.extend(neg_label)\n",
    "    total_label = pos_label\n",
    "    pair_data = pd.DataFrame(data={\n",
    "        'code1':total_code1,\n",
    "        'code2':total_code2,\n",
    "        'similar':total_label\n",
    "    })\n",
    "    pair_data = pair_data.sample(frac=1).reset_index(drop=True)\n",
    "    return pair_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eef45a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed():\n",
    "    random.seed(42)\n",
    "    os.environ['PYTHONHASHSEED'] = str(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6166c28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.06it/s]\n",
      "100%|██████████| 595000/595000 [01:43<00:00, 5736.94it/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (984 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 500/500 [2:25:11<00:00, 17.42s/it]  \n",
      "100%|██████████| 500/500 [12:23<00:00,  1.49s/it]\n"
     ]
    }
   ],
   "source": [
    "# 데이콘이 제공해준 학습 코드 데이터 데이터프레임 만들기\n",
    "code_folder = \"./train_code\"  # 데이콘이 제공해준 학습 데이터 파일의 경로\n",
    "problem_folders = os.listdir(code_folder)\n",
    "preproc_scripts = []\n",
    "problem_nums = []\n",
    "\n",
    "for problem_folder in tqdm(problem_folders):\n",
    "    scripts = os.listdir(os.path.join(code_folder, problem_folder))\n",
    "    problem_num = scripts[0].split('_')[0]\n",
    "    for script in scripts:\n",
    "        script_file = os.path.join(code_folder, problem_folder, script)\n",
    "        preprocessed_script = clean_data(script_file, data_type=\"dir\")\n",
    "        preproc_scripts.append(preprocessed_script)\n",
    "    problem_nums.extend([problem_num] * len(scripts))\n",
    "train_df = pd.DataFrame(data={'code': preproc_scripts, 'problem_num': problem_nums})\n",
    "\n",
    "# 데이콘이 제공해준 테스트 코드 데이터 데이터프레임 만들기\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "code1 = test_df['code1'].values\n",
    "code2 = test_df['code2'].values\n",
    "processed_code1 = []\n",
    "processed_code2 = []\n",
    "for i in tqdm(range(len(code1))):\n",
    "    processed_c1 = clean_data(code1[i], data_type=\"file\")\n",
    "    processed_c2 = clean_data(code2[i], data_type=\"file\")\n",
    "    processed_code1.append(processed_c1)\n",
    "    processed_code2.append(processed_c2)\n",
    "processed_test = pd.DataFrame(list(zip(processed_code1, processed_code2)), columns=[\"code1\", \"code2\"])\n",
    "\n",
    "# 데이터 프레임을 만들었으니 이제 train/val split을 진행하고, positive, negative pairs를 생성합니다.\n",
    "# 청소님의 코드를 참고해서 hard negative pair를 생성하였으며, BM25대신 BM25L을 사용합니다.\n",
    "# (BM25, BM25L 모두 테스트한 결과 BM25L에서 더 좋은 성능을 보였습니다.)\n",
    "# tokenizer는 왼쪽부터 truncation을 진행하여 truncation이 필요할때는 코드의 끝 부분들을 이용하게 만듭니다.\n",
    "\n",
    "dacon_train_df, dacon_valid_df, dacon_train_label, dacon_valid_label = train_test_split(\n",
    "    train_df,\n",
    "    train_df['problem_num'],\n",
    "    random_state=42,\n",
    "    test_size=0.1\n",
    ")\n",
    "\n",
    "dacon_train_df = dacon_train_df.reset_index(drop=True)\n",
    "dacon_valid_df = dacon_valid_df.reset_index(drop=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "tokenizer.truncation_side = 'left'\n",
    "\n",
    "dacon_train_bm25L = get_pairs(dacon_train_df, tokenizer)\n",
    "dacon_valid_bm25L = get_pairs(dacon_valid_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4bb0571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성된 데이터를 저장합니다. => 이 과정까지의 생성 시간이 꽤 오래걸립니다.\n",
    "dacon_train_bm25L.to_csv(\"train_bm25L.csv\", index=False)\n",
    "dacon_valid_bm25L.to_csv(\"valid_bm25L.csv\", index=False)\n",
    "processed_test.to_csv(\"processed_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3d21e44",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 385. GiB for an array with shape (100988696, 512) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-ecc7b8189030>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mMAX_LEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mattention_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 385. GiB for an array with shape (100988696, 512) and data type int64"
     ]
    }
   ],
   "source": [
    "set_seed()\n",
    "\n",
    "dacon_train_bm25L = pd.read_csv(\"train_bm25L.csv\")\n",
    "dacon_valid_bm25L = pd.read_csv(\"valid_bm25L.csv\")\n",
    "\n",
    "train_data = dacon_train_bm25L\n",
    "valid_data = dacon_valid_bm25L\n",
    "\n",
    "# training\n",
    "c1 = train_data['code1'].values\n",
    "c2 = train_data['code2'].values\n",
    "similar = train_data['similar'].values\n",
    "\n",
    "N = train_data.shape[0]\n",
    "MAX_LEN = 512\n",
    "\n",
    "input_ids = np.zeros((N, MAX_LEN), dtype=int)\n",
    "attention_masks = np.zeros((N, MAX_LEN), dtype=int)\n",
    "labels = np.zeros((N), dtype=int)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "    \n",
    "for i in tqdm(range(N), position=0, leave=True):\n",
    "    try:\n",
    "        cur_c1 = str(c1[i])\n",
    "        cur_c2 = str(c2[i])\n",
    "        encoded_input = tokenizer(cur_c1, cur_c2, return_tensors='pt', max_length=512, padding='max_length',\n",
    "                                    truncation=True)\n",
    "        input_ids[i,] = encoded_input['input_ids']\n",
    "        attention_masks[i,] = encoded_input['attention_mask']\n",
    "        labels[i] = similar[i]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "\n",
    "# validating\n",
    "c1 = valid_data['code1'].values\n",
    "c2 = valid_data['code2'].values\n",
    "similar = valid_data['similar'].values\n",
    "\n",
    "N = valid_data.shape[0]\n",
    "\n",
    "MAX_LEN = 512\n",
    "\n",
    "valid_input_ids = np.zeros((N, MAX_LEN), dtype=int)\n",
    "valid_attention_masks = np.zeros((N, MAX_LEN), dtype=int)\n",
    "valid_labels = np.zeros((N), dtype=int)\n",
    "\n",
    "for i in tqdm(range(N), position=0, leave=True):\n",
    "    try:\n",
    "        cur_c1 = str(c1[i])\n",
    "        cur_c2 = str(c2[i])\n",
    "        encoded_input = tokenizer(cur_c1, cur_c2, return_tensors='pt', max_length=512, padding='max_length',\n",
    "                                    truncation=True)\n",
    "        valid_input_ids[i,] = encoded_input['input_ids']\n",
    "        valid_attention_masks[i,] = encoded_input['attention_mask']\n",
    "        valid_labels[i] = similar[i]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "if os.path.exists(\"graphcodebert\"):\n",
    "    os.makedirs(\"graphcodebert\", exist_ok=True)\n",
    "\n",
    "print(\"\\n\\nMake tensor\\n\\n\")\n",
    "input_ids = torch.tensor(input_ids, dtype=int)\n",
    "attention_masks = torch.tensor(attention_masks, dtype=int)\n",
    "labels = torch.tensor(labels, dtype=int)\n",
    "\n",
    "valid_input_ids = torch.tensor(valid_input_ids, dtype=int)\n",
    "valid_attention_masks = torch.tensor(valid_attention_masks, dtype=int)\n",
    "valid_labels = torch.tensor(valid_labels, dtype=int)\n",
    "\n",
    "save_tensor = True\n",
    "if save_tensor == True:\n",
    "    torch.save(input_ids, \"\" + 'train_input_ids_BM25L.pt')\n",
    "    torch.save(attention_masks, \"\" + 'train_attention_masks_BM25L.pt')\n",
    "    torch.save(labels, \"\" + 'train_labels_BM25L.pt')\n",
    "\n",
    "    torch.save(valid_input_ids, \"\" + \"valid_input_ids_BM25L.pt\")\n",
    "    torch.save(valid_attention_masks, \"\" + \"valid_attention_masks_BM25L.pt\")\n",
    "    torch.save(valid_labels, \"\" + \"valid_labels_BM25L.pt\")\n",
    "\n",
    "\n",
    "# Setup training\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "train_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "validation_data = TensorDataset(valid_input_ids, valid_attention_masks, valid_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=32)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "model.cuda()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-5)\n",
    "\n",
    "total_steps = len(train_dataloader) * 3\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train\n",
    "train_losses, train_accuracies = [], []\n",
    "val_losses, val_accuracies = [], []\n",
    "model.zero_grad()\n",
    "for i in range(3):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(i + 1, 3))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    train_loss, train_accuracy = 0, 0\n",
    "    model.train()\n",
    "    for step, batch in tqdm(enumerate(train_dataloader), desc=\"Iteration\", smoothing=0.05):\n",
    "        if step % 10000 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            print('  current average loss = {}'.format(\n",
    "                train_loss / step))  # bot.sendMessage(chat_id=chat_id, text = '  current average loss = {}'.format(train_loss / step))\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        train_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.detach().cpu().numpy()\n",
    "        train_accuracy += flat_accuracy(logits, label_ids)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    avg_train_accuracy = train_accuracy / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(avg_train_accuracy)\n",
    "    print(\"  Average training loss: {0:.8f}\".format(avg_train_loss))\n",
    "    print(\"  Average training accuracy: {0:.8f}\".format(avg_train_accuracy))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Validating...\")\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    val_loss, val_accuracy = 0, 0\n",
    "    for step, batch in tqdm(enumerate(validation_dataloader), desc=\"Iteration\", smoothing=0.05):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu()\n",
    "        label_ids = b_labels.detach().cpu()\n",
    "        val_loss += loss_f(logits, label_ids)\n",
    "\n",
    "        logits = logits.numpy()\n",
    "        label_ids = label_ids.numpy()\n",
    "        val_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = val_accuracy / len(validation_dataloader)\n",
    "    avg_val_loss = val_loss / len(validation_dataloader)\n",
    "    val_accuracies.append(avg_val_accuracy)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(\"  Average validation loss: {0:.8f}\".format(avg_val_loss))\n",
    "    print(\"  Average validation accuracy: {0:.8f}\".format(avg_val_accuracy))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    # if np.min(val_losses) == val_losses[-1]:\n",
    "    print(\"saving current best checkpoint\")\n",
    "    torch.save(model.state_dict(), \"\" + str(i + 1) + \"_BM25L.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b60b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data = pd.read_csv(\"processed_test.csv\")\n",
    "\n",
    "c1 = test_data['code1'].values\n",
    "c2 = test_data['code2'].values\n",
    "\n",
    "N = test_data.shape[0]\n",
    "MAX_LEN = 1024\n",
    "\n",
    "test_input_ids = np.zeros((N, MAX_LEN), dtype=int)\n",
    "test_attention_masks = np.zeros((N, MAX_LEN), dtype=int)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "tokenizer.truncation_side = \"left\"\n",
    "\n",
    "for i in tqdm(range(N), position=0, leave=True):\n",
    "    try:\n",
    "        cur_c1 = str(c1[i])\n",
    "        cur_c2 = str(c2[i])\n",
    "        encoded_input = tokenizer(cur_c1, cur_c2, return_tensors='pt', max_length=1024, padding='max_length',\n",
    "                                    truncation=True)\n",
    "        test_input_ids[i,] = encoded_input['input_ids']\n",
    "        test_attention_masks[i,] = encoded_input['attention_mask']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "test_input_ids = torch.tensor(test_input_ids, dtype=int)\n",
    "test_attention_masks = torch.tensor(test_attention_masks, dtype=int)\n",
    "\n",
    "if save_tensor == True:\n",
    "    torch.save(test_input_ids, \"\" + \"test_input_ids.pt\")\n",
    "    torch.save(test_attention_masks, \"\" + \"test_attention_masks.pt\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "PATH = \"\" + \"4\" + \"+_BM25L.pt\"\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.cuda()\n",
    "\n",
    "test_tensor = TensorDataset(test_input_ids, test_attention_masks)\n",
    "test_sampler = SequentialSampler(test_tensor)\n",
    "test_dataloader = DataLoader(test_tensor, sampler=test_sampler, batch_size=1048)\n",
    "\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "preds = np.array([])\n",
    "for step, batch in tqdm(enumerate(test_dataloader), desc=\"Iteration\", smoothing=0.05):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu()\n",
    "    _pred = logits.numpy()\n",
    "    pred = np.argmax(_pred, axis=1).flatten()\n",
    "    preds = np.append(preds, pred)\n",
    "\n",
    "submission['similar'] = preds\n",
    "submission.to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5d1ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def model_ensemble():\n",
    "#     submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "#     submission_1 = pd.read_csv('submission_graphcodebert_BM25L.csv')\n",
    "#     submission_2 = pd.read_csv('submission_CodeBERTaPy_BM25L.csv')\n",
    "#     submission_3 = pd.read_csv('submission_codebert_mlm_BM25L.csv')\n",
    "\n",
    "#     sub_1 = submission_1['similar']\n",
    "#     sub_2 = submission_2['similar']\n",
    "#     sub_3 = submission_3['similar']\n",
    "\n",
    "#     ensemble_preds = (sub_1 + sub_2 + sub_3) / 3\n",
    "\n",
    "#     preds = np.where(ensemble_preds > 0.5, 1, 0)\n",
    "\n",
    "#     submission['similar'] = preds\n",
    "\n",
    "#     submission.to_csv('submission_ensemble_v2.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
